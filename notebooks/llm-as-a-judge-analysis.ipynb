{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d526d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_suffix = \"translation\"\n",
    "paraphrase_suffix = \"paraphrase\"\n",
    "\n",
    "translation_files = []\n",
    "paraphrase_files = {\n",
    "    \"de\": [],\n",
    "    \"en\": [],\n",
    "    \"fa\": [],\n",
    "    \"id\": [],\n",
    "    \"ja\": [],\n",
    "    \"zh\": [],\n",
    "    \"ar\": [],\n",
    "}\n",
    "languages = [\"de\", \"en\", \"fa\", \"id\", \"ja\", \"zh\", \"ar\"]\n",
    "for seed in [42, 123]:\n",
    "    translation_files.append(f\"../outputs/llm-as-a-judge/gpt_bias_results_seed_{seed}_{translation_suffix}.jsonl\")\n",
    "\n",
    "for seed in [42, 123]:\n",
    "    for language in languages:\n",
    "        paraphrase_files[language].append(f\"../outputs/llm-as-a-judge/{language}_gpt_bias_results_seed_{seed}_{paraphrase_suffix}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_files, paraphrase_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de1e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation data: 2 entries\n",
      "Paraphrase data: 7 entries\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"translation\": [],\n",
    "    \"paraphrase\": {\n",
    "        \"de\": [],\n",
    "        \"en\": [],\n",
    "        \"fa\": [],\n",
    "        \"id\": [],\n",
    "        \"ja\": [],\n",
    "        \"zh\": [],\n",
    "        \"ar\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "for file in translation_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data[\"translation\"].append(json.load(f))\n",
    "\n",
    "for language in languages:\n",
    "    for file in paraphrase_files[language]:\n",
    "        with open(file, \"r\") as f:\n",
    "            data[\"paraphrase\"][language].append(json.load(f))\n",
    "\n",
    "# Check if the data is loaded correctly\n",
    "print(f\"Translation data: {len(data['translation'])} entries\")\n",
    "print(f\"Paraphrase data: {len(data['paraphrase'])} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15164276",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pattern = re.compile(\n",
    "    r\"\\*\\*([A-Z][a-z]+) Text.*?\\*\\*.*?- Depth of detail: (\\d+).*?- Clarity of writing: (\\d+).*?- Coherence and logical flow: (\\d+).*?- Originality and insight: (\\d+).*?- Use of specific examples: (\\d+).*?- Accuracy of information: (\\d+)\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "\n",
    "scores_pattern_paraphrase = re.compile(\n",
    "    r\"\\*\\*(Text [AB])(?: Evaluations)?:\\*\\*\\s*\"\n",
    "    r\"1\\. ?\\**Depth of detail\\**?: (\\d+) -.*?\"\n",
    "    r\"2\\. ?\\**Clarity of writing\\**?: (\\d+) -.*?\"\n",
    "    r\"3\\. ?\\**Coherence and logical flow\\**?: (\\d+) -.*?\"\n",
    "    r\"4\\. ?\\**Originality and insight\\**?: (\\d+) -.*?\"\n",
    "    r\"5\\. ?\\**Use of specific examples\\**?: (\\d+) -.*?\"\n",
    "    r\"6\\. ?\\**Accuracy of information\\**?: (\\d+)\",\n",
    "    re.DOTALL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0389a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_judge_data_translation = []\n",
    "\n",
    "for file_index, file in enumerate(data[\"translation\"]):\n",
    "    temp_list = []\n",
    "    for data_index, element in enumerate(file):\n",
    "        shuffled_languages = element[\"shuffled_languages\"]\n",
    "        matches = scores_pattern.findall(element[\"result\"])\n",
    "        temp_list.append({\"index\": data_index, \"language-order\": shuffled_languages, \"scores\": [[int(score) for score in match[1:]] for match in matches], \"verdict\": element[\"verdict\"]})\n",
    "    gpt_judge_data_translation.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_judge_data_paraphrase = {\n",
    "    \"de\": [],\n",
    "    \"en\": [],\n",
    "    \"fa\": [],\n",
    "    \"id\": [],\n",
    "    \"ja\": [],\n",
    "    \"zh\": [],\n",
    "    \"ar\": [],\n",
    "}\n",
    "\n",
    "for language in languages:\n",
    "    for file_index, file in enumerate(data[\"paraphrase\"][language]):\n",
    "        individual_run = []\n",
    "        for data_index, element in enumerate(file):\n",
    "            shuffled_languages = element[\"shuffled_languages\"]\n",
    "            matches = scores_pattern_paraphrase.findall(element[\"result\"])\n",
    "            individual_run.append({\"index\": data_index, \"language-order\": shuffled_languages, \"scores\": [[int(score) for score in match[1:]] for match in matches], \"verdict\": element[\"verdict\"]})\n",
    "        gpt_judge_data_paraphrase[language].append(individual_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac0e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_judge_data_paraphrase['en'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2acef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, variance, stdev\n",
    "\n",
    "# Calculate individual run results\n",
    "run_results = []\n",
    "for run in gpt_judge_data_translation:\n",
    "    run_verdict_counts = Counter([entry['verdict'] for entry in run])\n",
    "    total_count_run = sum(run_verdict_counts.values())\n",
    "    run_percentages = {k: v / total_count_run * 100 for k, v in run_verdict_counts.items()}\n",
    "    run_results.append(run_percentages)\n",
    "\n",
    "# Calculate averages, variance, and standard deviation across runs\n",
    "all_verdicts = [entry['verdict'] for run in gpt_judge_data_translation for entry in run]\n",
    "verdict_counts_all = Counter(all_verdicts)\n",
    "total_count_all = sum(verdict_counts_all.values())\n",
    "verdict_percentages_all = {k: v / total_count_all * 100 for k, v in verdict_counts_all.items()}\n",
    "\n",
    "averages = {k: mean([run.get(k, 0) for run in run_results]) for k in verdict_percentages_all.keys()}\n",
    "variances = {k: variance([run.get(k, 0) for run in run_results]) for k in verdict_percentages_all.keys()}\n",
    "std_devs = {k: stdev([run.get(k, 0) for run in run_results]) for k in verdict_percentages_all.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcd99f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Run Results:\n",
      "Run 1:\n",
      " EN: 23.00%\n",
      " JA: 17.00%\n",
      " FA: 2.00%\n",
      " AR: 19.00%\n",
      " ZH: 8.00%\n",
      " DE: 28.00%\n",
      " ID: 3.00%\n",
      "Run 2:\n",
      " EN: 16.00%\n",
      " JA: 12.00%\n",
      " DE: 33.00%\n",
      " AR: 27.00%\n",
      " ID: 4.00%\n",
      " ZH: 6.00%\n",
      " FA: 2.00%\n",
      "\n",
      "Overall Statistics:\n",
      "Averages: {'EN': 19.5, 'JA': 14.5, 'FA': 2.0, 'AR': 23.0, 'ZH': 7.0, 'DE': 30.5, 'ID': 3.5}\n",
      "Variances: {'EN': 24.5, 'JA': 12.5, 'FA': 0.0, 'AR': 32.0, 'ZH': 2.0, 'DE': 12.499999999999982, 'ID': 0.5}\n",
      "Standard Deviations: {'EN': 4.949747468305833, 'JA': 3.5355339059327378, 'FA': 0.0, 'AR': 5.656854249492381, 'ZH': 1.4142135623730951, 'DE': 3.535533905932735, 'ID': 0.7071067811865476}\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Individual Run Results:\")\n",
    "for i, run in enumerate(run_results):\n",
    "    print(f\"Run {i + 1}:\")\n",
    "    for verdict, percentage in run.items():\n",
    "        print(f\" {verdict}: {percentage:.2f}%\")\n",
    "    # print(f\"Run {i + 1}: {run}\")\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(\"Averages:\", averages)\n",
    "print(\"Variances:\", variances)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "\n",
    "# verdict_counter_first_last = Counter(\n",
    "#     [entry['verdict'] for element in gpt_judge_data_translation for entry in element if entry['language-order'].index(entry['verdict'].lower()) in [0, len(entry['language-order']) - 1]]\n",
    "# )\n",
    "# verdict_counts_translation = Counter([entry['verdict'] for element in gpt_judge_data_translation for entry in element])\n",
    "# # Calculate the percentage for each language\n",
    "# total_count_translation = sum(verdict_counts_translation.values())\n",
    "# verdict_percentages_translation = {k: v / total_count_translation * 100 for k, v in verdict_counts_translation.items()}\n",
    "# verdict_percentages_first_last = {k: v / total_count_translation * 100 for k, v in verdict_counter_first_last.items()}\n",
    "# print(\"Translation Verdict Percentages:\")\n",
    "# print(\"Language: Percentage - Count_first_last\")\n",
    "# for verdict, percentage in verdict_percentages_translation.items():\n",
    "#     print(f\"{verdict}: {percentage:.2f}% - {verdict_percentages_first_last.get(verdict, 0):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7680b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Run Results:\n",
      "Language: de\n",
      " Run 1:\n",
      "  perturbed_text: 52\n",
      "  TIE: 17\n",
      "  natural_text: 30\n",
      "  Model Failure: 1\n",
      " Run 2:\n",
      "  perturbed_text: 43\n",
      "  natural_text: 27\n",
      "  TIE: 29\n",
      "  Model Failure: 1\n",
      "Language: en\n",
      " Run 1:\n",
      "  perturbed_text: 82\n",
      "  natural_text: 17\n",
      "  TIE: 1\n",
      " Run 2:\n",
      "  perturbed_text: 69\n",
      "  natural_text: 31\n",
      "Language: fa\n",
      " Run 1:\n",
      "  natural_text: 23\n",
      "  TIE: 40\n",
      "  perturbed_text: 37\n",
      " Run 2:\n",
      "  perturbed_text: 42\n",
      "  TIE: 41\n",
      "  natural_text: 17\n",
      "Language: id\n",
      " Run 1:\n",
      "  TIE: 19\n",
      "  natural_text: 36\n",
      "  perturbed_text: 45\n",
      " Run 2:\n",
      "  TIE: 24\n",
      "  perturbed_text: 51\n",
      "  natural_text: 25\n",
      "Language: ja\n",
      " Run 1:\n",
      "  TIE: 33\n",
      "  natural_text: 28\n",
      "  perturbed_text: 39\n",
      " Run 2:\n",
      "  natural_text: 29\n",
      "  TIE: 42\n",
      "  perturbed_text: 29\n",
      "Language: zh\n",
      " Run 1:\n",
      "  perturbed_text: 31\n",
      "  TIE: 43\n",
      "  natural_text: 26\n",
      " Run 2:\n",
      "  perturbed_text: 29\n",
      "  TIE: 45\n",
      "  natural_text: 26\n",
      "Language: ar\n",
      " Run 1:\n",
      "  natural_text: 35\n",
      "  TIE: 28\n",
      "  perturbed_text: 37\n",
      " Run 2:\n",
      "  perturbed_text: 44\n",
      "  natural_text: 26\n",
      "  TIE: 30\n",
      "Language: de\n",
      " Averages: {'TIE': 23.0, 'perturbed_text': 47.5, 'Model Failure': 1.0, 'natural_text': 28.5}\n",
      " Variances: {'TIE': 71.99999999999996, 'perturbed_text': 40.5, 'Model Failure': 0.0, 'natural_text': 4.5}\n",
      " Standard Deviations: {'TIE': 8.485281374238568, 'perturbed_text': 6.363961030678928, 'Model Failure': 0.0, 'natural_text': 2.1213203435596424}\n",
      "Language: en\n",
      " Averages: {'TIE': 0.5, 'perturbed_text': 75.5, 'natural_text': 24.0}\n",
      " Variances: {'TIE': 0.5, 'perturbed_text': 84.5, 'natural_text': 98.0}\n",
      " Standard Deviations: {'TIE': 0.7071067811865476, 'perturbed_text': 9.192388155425117, 'natural_text': 9.899494936611665}\n",
      "Language: fa\n",
      " Averages: {'TIE': 40.5, 'perturbed_text': 39.5, 'natural_text': 20.0}\n",
      " Variances: {'TIE': 0.5, 'perturbed_text': 12.5, 'natural_text': 18.0}\n",
      " Standard Deviations: {'TIE': 0.7071067811865476, 'perturbed_text': 3.5355339059327378, 'natural_text': 4.242640687119285}\n",
      "Language: id\n",
      " Averages: {'TIE': 21.5, 'perturbed_text': 48.0, 'natural_text': 30.5}\n",
      " Variances: {'TIE': 12.5, 'perturbed_text': 18.0, 'natural_text': 60.5}\n",
      " Standard Deviations: {'TIE': 3.5355339059327378, 'perturbed_text': 4.242640687119285, 'natural_text': 7.7781745930520225}\n",
      "Language: ja\n",
      " Averages: {'TIE': 37.5, 'perturbed_text': 34.0, 'natural_text': 28.5}\n",
      " Variances: {'TIE': 40.5, 'perturbed_text': 50.000000000000036, 'natural_text': 0.4999999999999929}\n",
      " Standard Deviations: {'TIE': 6.363961030678928, 'perturbed_text': 7.071067811865478, 'natural_text': 0.7071067811865425}\n",
      "Language: zh\n",
      " Averages: {'TIE': 44.0, 'perturbed_text': 30.0, 'natural_text': 26.0}\n",
      " Variances: {'TIE': 2.0, 'perturbed_text': 2.000000000000007, 'natural_text': 0.0}\n",
      " Standard Deviations: {'TIE': 1.4142135623730951, 'perturbed_text': 1.4142135623730976, 'natural_text': 0.0}\n",
      "Language: ar\n",
      " Averages: {'TIE': 29.0, 'perturbed_text': 40.5, 'natural_text': 30.5}\n",
      " Variances: {'TIE': 1.999999999999993, 'perturbed_text': 24.5, 'natural_text': 40.5}\n",
      " Standard Deviations: {'TIE': 1.4142135623730925, 'perturbed_text': 4.949747468305833, 'natural_text': 6.363961030678928}\n"
     ]
    }
   ],
   "source": [
    "# Calculate counts for individual runs\n",
    "individual_run_counts_paraphrase = {\n",
    "    language: [\n",
    "        Counter([entry['verdict'] for entry in gpt_judge_data_paraphrase[language][0]]),\n",
    "        Counter([entry['verdict'] for entry in gpt_judge_data_paraphrase[language][1]])\n",
    "    ]\n",
    "    for language in languages\n",
    "}\n",
    "\n",
    "# Calculate statistics for each language\n",
    "statistics_paraphrase = {}\n",
    "for language in languages:\n",
    "    run_totals = [\n",
    "        sum(individual_run_counts_paraphrase[language][0].values()),\n",
    "        sum(individual_run_counts_paraphrase[language][1].values())\n",
    "    ]\n",
    "    run_percentages = [\n",
    "        {k: v / run_totals[0] * 100 for k, v in individual_run_counts_paraphrase[language][0].items()},\n",
    "        {k: v / run_totals[1] * 100 for k, v in individual_run_counts_paraphrase[language][1].items()}\n",
    "    ]\n",
    "    all_keys = set(run_percentages[0].keys()).union(run_percentages[1].keys())\n",
    "    averages = {k: mean([run_percentages[0].get(k, 0), run_percentages[1].get(k, 0)]) for k in all_keys}\n",
    "    variances = {k: variance([run_percentages[0].get(k, 0), run_percentages[1].get(k, 0)]) for k in all_keys}\n",
    "    std_devs = {k: stdev([run_percentages[0].get(k, 0), run_percentages[1].get(k, 0)]) for k in all_keys}\n",
    "    statistics_paraphrase[language] = {\n",
    "        \"averages\": averages,\n",
    "        \"variances\": variances,\n",
    "        \"std_devs\": std_devs\n",
    "    }\n",
    "\n",
    "print(\"Individual Run Results:\")\n",
    "for language in languages:\n",
    "    print(f\"Language: {language}\")\n",
    "    for i, run in enumerate(individual_run_counts_paraphrase[language]):\n",
    "        print(f\" Run {i + 1}:\")\n",
    "        for verdict, count in run.items():\n",
    "            print(f\"  {verdict}: {count}\")\n",
    "            \n",
    "# Print statistics\n",
    "for language, stats in statistics_paraphrase.items():\n",
    "    print(f\"Language: {language}\")\n",
    "    print(\" Averages:\", stats[\"averages\"])\n",
    "    print(\" Variances:\", stats[\"variances\"])\n",
    "    print(\" Standard Deviations:\", stats[\"std_devs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d34ce6",
   "metadata": {},
   "source": [
    "Overall Statistics:\n",
    "Averages: {'EN': 19.5, 'JA': 14.5, 'FA': 2.0, 'AR': 23.0, 'ZH': 7.0, 'DE': 30.5, 'ID': 3.5}\n",
    "Variances: {'EN': 24.5, 'JA': 12.5, 'FA': 0.0, 'AR': 32.0, 'ZH': 2.0, 'DE': 12.499999999999982, 'ID': 0.5}\n",
    "Standard Deviations: {'EN': 4.949747468305833, 'JA': 3.5355339059327378, 'FA': 0.0, 'AR': 5.656854249492381, 'ZH': 1.4142135623730951, 'DE': 3.535533905932735, 'ID': 0.7071067811865476}\n",
    "\n",
    "### Translation Verdict Percentages\n",
    "| Language | Percentage (%) | Count_first_last (%) |\n",
    "|----------|:-------------:|:--------------------:|\n",
    "| EN       | 19.50 ± 4.95  | 6.50         |\n",
    "| JA       | 14.50 ± 3.54  | 13.00        |\n",
    "| FA       | 2.00 ± 0.00   | 2.00         |\n",
    "| AR       | 23.00 ± 5.66  | 13.00        |\n",
    "| ZH       | 7.00 ± 1.41   | 6.50         |\n",
    "| DE       | 30.50 ± 3.54  | 11.00        |\n",
    "| ID       | 3.50 ± 0.71   | 3.50         |\n",
    "\n",
    "\n",
    "### Paraphrase Verdict Percentages\n",
    "\n",
    "| Language | perturbed_text (%) | natural_text (%) | TIE (%) | Model Failure (%) |\n",
    "|----------|:-----------------:|:---------------:|:-------:|:-----------------:|\n",
    "| zh       | 30.00             | 26.00           | 44.00   |                   |\n",
    "| fa       | 39.50             | 20.00           | 40.50   |                   |\n",
    "| ja       | 34.00             | 28.50           | 37.50   |                   |\n",
    "| ar       | 40.50             | 30.50           | 29.00   |                   |\n",
    "| de       | 47.50             | 28.50           | 23.00   | 1.00              |\n",
    "| id       | 48.00             | 30.50           | 21.50   |                   |\n",
    "| en       | 75.50             | 24.00           | 0.50    |                   |\n",
    "\n",
    "### Individual Runs Paraphrase:\n",
    "\n",
    "### Individual Run Results:\n",
    "\n",
    "| Language | Run | Perturbed Text (%) | Natural Text (%) | TIE (%) | Model Failure (%) |\n",
    "|----------|-----|--------------------|------------------|---------|-------------------|\n",
    "| de       | 1   | 52                 | 30               | 17      | 1                 |\n",
    "| de       | 2   | 43                 | 27               | 29      | 1                 |\n",
    "| en       | 1   | 82                 | 17               | 1       |                   |\n",
    "| en       | 2   | 69                 | 31               | 0       |                   |\n",
    "| fa       | 1   | 37                 | 23               | 40      |                   |\n",
    "| fa       | 2   | 42                 | 17               | 41      |                   |\n",
    "| id       | 1   | 45                 | 36               | 19      |                   |\n",
    "| id       | 2   | 51                 | 25               | 24      |                   |\n",
    "| ja       | 1   | 39                 | 28               | 33      |                   |\n",
    "| ja       | 2   | 29                 | 29               | 42      |                   |\n",
    "| zh       | 1   | 31                 | 26               | 43      |                   |\n",
    "| zh       | 2   | 29                 | 26               | 45      |                   |\n",
    "| ar       | 1   | 37                 | 35               | 28      |                   |\n",
    "| ar       | 2   | 44                 | 26               | 30      |                   |\n",
    "\n",
    "\n",
    "### Paraphrase Statistics by Language\n",
    "| Language | TIE (%)          | Perturbed Text (%)   | Natural Text (%)     | Model Failure (%) |\n",
    "|----------|------------------|---------------------|---------------------|-------------------|\n",
    "| de       | 23.0 ± 8.49      | 47.5 ± 6.36         | 28.5 ± 2.12         | 1.0               |\n",
    "| en       | 0.5 ± 0.71       | 75.5 ± 9.19         | 24.0 ± 9.90         |                   |\n",
    "| fa       | 40.5 ± 0.71      | 39.5 ± 3.54         | 20.0 ± 4.24         |                   |\n",
    "| id       | 21.5 ± 3.54      | 48.0 ± 4.24         | 30.5 ± 7.78         |                   |\n",
    "| ja       | 37.5 ± 6.36      | 34.0 ± 7.07         | 28.5 ± 0.71         |                   |\n",
    "| zh       | 44.0 ± 1.41      | 30.0 ± 1.41         | 26.0 ± 0.0          |                   |\n",
    "| ar       | 29.0 ± 1.41      | 40.5 ± 4.95         | 30.5 ± 6.36         |                   |\n",
    "\n",
    "\n",
    "## Observations\n",
    "From traslation results, we see that Persian, Chinese, and Indonesian languages receive the lowest scores in terms of quality although all language texts used are translations from the original English texts. Previous studies indicated bias in llms-as-a-judge methods such as [1-5]. In order to try to get an unbaised judgement as much as possible we randomized the order in which texts are inputted to the text and we experiment with two runs with different seeds for 100 examples. Notice that, in the translation results, there are no TIEs, which indicates large confidence in choosing a language of the winning texts. German, English and Arabic seem to be the once favored the most by GPT-4o-mini.\n",
    "\n",
    "Studies also indicate [4] that LLMs can favor texts generated by a variant of the same judging model. We used GPT-3.5-turbo to translated from English to all languages, and we used GPT-4o-mini for judgements. The reason Arabic language was chosen more than the other intenationl langauges like Persian (same language family but with less scores) maybe attributed to translations intricacies by the translator which made the text easier for GPT-4o-mini to judge. To get a closer look and more understanding at this type of bias, we also did experiments of the same text judging. However, we want to make sure that the both texts under judging have the same meaning and closer lengths. For this purpose we introduce the results in the second table which is about judging two texts that are the same but one of them is a paraphrase of the other.\n",
    "\n",
    "In the paraphrase experiments, we say that when the model has more TIEs then it might not be confident about one text being better than the other. Comparing the paraphrase results with that of the translations, we noticed that the languages that received lower scores in transation tables are at the top of the paraphrase tables in terms of the TIE scores except for the Indoneisan language. In Indonesian language, the alphabet used are the same as English's, which might indicate a bias called token bias as indicated in this work [1]. Another observation is that perturbed text is favored more by LLM judger. This just confirms studies that indicate LLMs favoring texts from their own generations or from a variant model.\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "[1] Zheng, C., Zhou, H., Meng, F., Zhou, J., & Huang, M. Large language models are not robust multiple choice selectors, 2024. URL https://arxiv. org/abs/2309.03882. (token bias)\n",
    "\n",
    "[2] Koo, R., Lee, M., Raheja, V., Park, J. I., Kim, Z. M., & Kang, D. (2023). Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012. (benchmark biases)\n",
    "\n",
    "[3] https://www.simonpcouch.com/blog/2025-01-30-llm-biases/ (blog)\n",
    "\n",
    "[4] Ye, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N., ... & Zhang, X. (2024). Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736. (biases including LLM favoring own generations)\n",
    "\n",
    "[5] Pezeshkpour, P., & Hruschka, E. (2023). Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483. (position bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0e5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
